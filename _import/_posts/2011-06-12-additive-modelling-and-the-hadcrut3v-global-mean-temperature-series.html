--- 
title: Additive modelling and the HadCRUT3v global mean temperature series
status: publish
layout: post
published: true
meta: 
  tagazine-media: a:7:{s:7:"primary";s:85:"http://ucfagls.files.wordpress.com/2011/06/fitted_additive_model_plus_derivatives.png";s:6:"images";a:8:{s:75:"http://ucfagls.files.wordpress.com/2011/06/global_hadcrut3v_time_series.png";a:6:{s:8:"file_url";s:75:"http://ucfagls.files.wordpress.com/2011/06/global_hadcrut3v_time_series.png";s:5:"width";s:3:"600";s:6:"height";s:3:"400";s:4:"type";s:5:"image";s:4:"area";s:6:"240000";s:9:"file_path";s:0:"";}s:74:"http://ucfagls.files.wordpress.com/2011/06/fitted_gamm_ar1_smooth_plot.png";a:6:{s:8:"file_url";s:74:"http://ucfagls.files.wordpress.com/2011/06/fitted_gamm_ar1_smooth_plot.png";s:5:"width";s:3:"600";s:6:"height";s:3:"400";s:4:"type";s:5:"image";s:4:"area";s:6:"240000";s:9:"file_path";s:0:"";}s:74:"http://ucfagls.files.wordpress.com/2011/06/fitted_gamm_ar1_diagnostics.png";a:6:{s:8:"file_url";s:74:"http://ucfagls.files.wordpress.com/2011/06/fitted_gamm_ar1_diagnostics.png";s:5:"width";s:3:"600";s:6:"height";s:3:"400";s:4:"type";s:5:"image";s:4:"area";s:6:"240000";s:9:"file_path";s:0:"";}s:72:"http://ucfagls.files.wordpress.com/2011/06/comparison_of_gamm_models.png";a:6:{s:8:"file_url";s:72:"http://ucfagls.files.wordpress.com/2011/06/comparison_of_gamm_models.png";s:5:"width";s:3:"600";s:6:"height";s:3:"400";s:4:"type";s:5:"image";s:4:"area";s:6:"240000";s:9:"file_path";s:0:"";}s:74:"http://ucfagls.files.wordpress.com/2011/06/derivatives_of_fitted_model.png";a:6:{s:8:"file_url";s:74:"http://ucfagls.files.wordpress.com/2011/06/derivatives_of_fitted_model.png";s:5:"width";s:3:"600";s:6:"height";s:3:"400";s:4:"type";s:5:"image";s:4:"area";s:6:"240000";s:9:"file_path";s:0:"";}s:85:"http://ucfagls.files.wordpress.com/2011/06/fitted_additive_model_plus_derivatives.png";a:6:{s:8:"file_url";s:85:"http://ucfagls.files.wordpress.com/2011/06/fitted_additive_model_plus_derivatives.png";s:5:"width";s:3:"600";s:6:"height";s:3:"400";s:4:"type";s:5:"image";s:4:"area";s:6:"240000";s:9:"file_path";s:0:"";}s:66:"http://ucfagls.files.wordpress.com/2011/06/25_simulated_trends.png";a:6:{s:8:"file_url";s:66:"http://ucfagls.files.wordpress.com/2011/06/25_simulated_trends.png";s:5:"width";s:3:"600";s:6:"height";s:3:"400";s:4:"type";s:5:"image";s:4:"area";s:6:"240000";s:9:"file_path";s:0:"";}s:77:"http://ucfagls.files.wordpress.com/2011/06/simulated_trends_recent_period.png";a:6:{s:8:"file_url";s:77:"http://ucfagls.files.wordpress.com/2011/06/simulated_trends_recent_period.png";s:5:"width";s:3:"600";s:6:"height";s:3:"400";s:4:"type";s:5:"image";s:4:"area";s:6:"240000";s:9:"file_path";s:0:"";}}s:6:"videos";a:0:{}s:11:"image_count";s:1:"8";s:6:"author";s:8:"15232487";s:7:"blog_id";s:8:"14744973";s:9:"mod_stamp";s:19:"2011-06-12 20:39:35";}
  _edit_last: "15232487"
  jabber_published: "1307910945"
type: post
tags: 
- Climate Change
- R
- Science
- Time series
---
<a href="http://ucfagls.wordpress.com/2011/06/11/global-warming-since-1995-now-significant/" title="Global warming since 1995 ‘now&nbsp;significant’">Earlier, I looked at the HadCRUT3vgl data</a> set using generalized least squares to investigate whether the trend in temperature since 1995 was statistically significant. Here I want to follow-up one of the points from the earlier posting; namely using a statistical technique that fits a local, and not global, model to the entire time series and see how that informs our knowledge of trends in the recent period.

<!--more-->

In this post, I'll be using the <strong>mgcv</strong> and <strong>nlme</strong> packages plus some custom functions I wrote to produce diagnostics plots of <code>gamm()</code> time series models and to compute derivatives of fitted splines using the method of finite differences. The latter can be loaded into R from my <a href="http://github.com" title="github website">github</a> <a href="https://github.com/gavinsimpson/random_code" title="My github repository">repository</a>

[sourcecode language="r" gutter="false"]
## load the packages and code we need
require(mgcv)
require(nlme)
## load custom functions
tmp &lt;- tempfile()
download.file(&quot;https://github.com/gavinsimpson/random_code/raw/master/derivFun.R&quot;,
              tmp, method = &quot;wget&quot;)
source(tmp)
tmp &lt;- tempfile()
download.file(&quot;https://github.com/gavinsimpson/random_code/raw/master/tsDiagGamm.R&quot;,
              tmp, method = &quot;wget&quot;)
source(tmp)
[/sourcecode]

(If the download code above doesn't work for you &mdash; it does on my Linux machine &mdash; then download the files using your browser and <code>source()</code> in the usual way.)

Next, load the data and process the file as per the earlier post (see <a href="http://ucfagls.wordpress.com/2011/06/11/global-warming-since-1995-now-significant/" title="Global warming since 1995 ‘now&nbsp;significant’">here</a> for details). The last lines of code plot the data (note that I only intend to use the annual means in this posting &mdash; dealing with monthly data needs a few extra steps to model the seasonal variation in the data).

[sourcecode language="r" gutter="false"]
## Global temperatures
URL &lt;- url(&quot;http://www.cru.uea.ac.uk/cru/data/temperature/hadcrut3vgl.txt&quot;)
gtemp &lt;- read.table(URL, fill = TRUE)
## Don't need the even rows
gtemp &lt;- gtemp[-seq(2, nrow(gtemp), by = 2), ]
## set the Year as rownames
rownames(gtemp) &lt;- gtemp[,1]
## Add colnames
colnames(gtemp) &lt;- c(&quot;Year&quot;, month.abb, &quot;Annual&quot;)
## Data for 2011 incomplete so work only with 1850-2010 data series
gtemp &lt;- gtemp[-nrow(gtemp), ]
## Plot the data
ylab &lt;- expression(Temperature~Anomaly~(1961-1990)~degree*C)
plot(Annual ~ Year, data = gtemp, type = &quot;o&quot;, ylab = ylab)
[/sourcecode]

The resulting plot should look like this:

[caption id="attachment_122" align="aligncenter" width="600" caption="Global mean temperature anomaly 1850-2010"]<a href="http://ucfagls.files.wordpress.com/2011/06/global_hadcrut3v_time_series.png"><img src="http://ucfagls.files.wordpress.com/2011/06/global_hadcrut3v_time_series.png" alt="Global mean temperature anomaly 1850-2010" title="Global mean temperature anomaly 1850-2010" width="600" height="400" class="size-full wp-image-122" /></a>[/caption]

Looking at the plot, we can see that the level of the global annual mean temperature record has varied substantially over the 160 years of observations. To fit a global, linear trend to the entire data would make little sense &mdash; clearly such a model would not provide a good fit to the data, failing to describe the relationship in temperature over time. Asking whether such a model is statistically significant is therefore moot. Instead, we want a model that can describe the changes in the underlying level. There are many such models, such as local linear smooths or loess smooths, but here I will use a thin-plate regression spline fitted using the <code>gamm()</code> function.

Why use a function that can fit generalized additive mixed models (GAMMs)? The sorts of additive models that can be fitted using <code>gam()</code> (note the one "m") can also be expressed as a linear mixed model, and the correlation structures I used in the earlier post can also be used in the <code>lme()</code> function, that fits linear mixed models. <code>gamm()</code> allows the two elements to be combined.

The additive model (without any correlation structure at this stage) is fitted and summarised as follows

[sourcecode language="r" gutter="false"]
&gt; ## Fit a smoother for Year to the data
&gt; m1 &lt;- gamm(Annual ~ s(Year, k = 20), data = gtemp)
&gt; summary(m1$gam)

Family: gaussian 
Link function: identity 

Formula:
Annual ~ s(Year, k = 20)

Parametric coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -0.165404   0.006972  -23.72   &lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

Approximate significance of smooth terms:
          edf Ref.df     F p-value    
s(Year) 11.94  11.94 101.3  &lt;2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

R-sq.(adj) =  0.883  Scale est. = 0.0077778  n = 161
[/sourcecode]

This smoother explains 88% of the variance in the data, uses almost 12 degrees of freedom and is statistically significant, in the sense that the fitted smoother is different from a null model. We should not take this <em>p</em>-value at face value however, as these data are a time series and the standard errors on the fitted smoother are likely to be overly narrow. The ACF and partial ACF can be used to determine what types of time series model might be required for the residuals

[sourcecode language="r" gutter="false"]
## look at autocorrelation in residuals:
acf(resid(m1$lme, type = &quot;normalized&quot;))
## ...wait... look at the plot, only then do...
pacf(resid(m1$lme, type = &quot;normalized&quot;))
## seems like like sharp cut-off in ACF and PACF - AR terms probably best
[/sourcecode]

Given the form of the ACF and pACF plots, AR terms will probably be best, so we fit models with AR(1) and AR(2) terms. To do this, we add a <code>correlation</code> argument to the model calls

[sourcecode language="r" gutter="false"]
## ...so fit the AR1
m2 &lt;- gamm(Annual ~ s(Year, k = 30), data = gtemp,
           correlation = corARMA(form = ~ Year, p = 1))
## ...and fit the AR2
m3 &lt;- gamm(Annual ~ s(Year, k = 30), data = gtemp,
           correlation = corARMA(form = ~ Year, p = 2))
[/sourcecode]

We can use the <code>anova()</code> method for <code>"lme"</code> objects to assess whether the models with the correlation structures fit the data better than the original model

[sourcecode language="r" gutter="false"]
&gt; anova(m1$lme, m2$lme, m3$lme)
       Model df       AIC       BIC   logLik   Test   L.Ratio p-value
m1$lme     1  4 -273.6235 -261.2978 140.8117                         
m2$lme     2  5 -299.7355 -284.3285 154.8678 1 vs 2 28.112063  &lt;.0001
m3$lme     3  6 -298.7174 -280.2290 155.3587 2 vs 3  0.981852  0.3217
[/sourcecode]

The AR(1) model provides the best fit to the data, being a significant improvement over the model without a correlation structure. AIC and BIC also both favour the AR(1) model over the AR(2) or the original model. The <code>plot()</code> method for <code>"gam"</code> objects can be used to view the fitted smoother; here I superimpose the residuals and alter the plotting character and size

[sourcecode language="r" gutter="false"]
plot(m2$gam, residuals = TRUE, pch = 19, cex = 0.75)
[/sourcecode]

to produce this plot

[caption id="attachment_148" align="aligncenter" width="600" caption="Fitted thin-plate spline with AR(1) residuals and approximate 95% point-wise confidence interval"]<a href="http://ucfagls.files.wordpress.com/2011/06/fitted_gamm_ar1_smooth_plot.png"><img src="http://ucfagls.files.wordpress.com/2011/06/fitted_gamm_ar1_smooth_plot.png" alt="Fitted thin-plate spline with AR(1) residuals and approximate 95% point-wise confidence interval" title="Fitted thin-plate spline with AR(1) residuals and approximate 95% point-wise confidence interval" width="600" height="400" class="size-full wp-image-148" /></a>[/caption]

Some diagnostic plots can be be produced using my <code>tsDiagGamm()</code> function (loaded earlier)

[sourcecode language="r" gutter="false"]
with(gtemp, tsDiagGamm(m2, timevar = Year, observed = Annual))
[/sourcecode]

which produces this figure:

[caption id="attachment_150" align="aligncenter" width="600" caption="Diagnostic plots for the fitted model using AR(1) residuals fitted to the 1850&ndash;2010 global mean annual temperature data"]<a href="http://ucfagls.files.wordpress.com/2011/06/fitted_gamm_ar1_diagnostics.png"><img src="http://ucfagls.files.wordpress.com/2011/06/fitted_gamm_ar1_diagnostics.png" alt="Diagnostic plots for the fitted model using AR(1) residuals fitted to the 1850&ndash;2010 global mean annual temperature data" title="Diagnostic plots for the fitted model using AR(1) residuals fitted to the 1850&ndash;2010 global mean annual temperature data" width="600" height="400" class="size-full wp-image-150" /></a>[/caption]

There do not seem to be any causes for concern in the diagnostics.

Finally, we can compare the fits of the original model and the model with AR(1) residuals. I use a general procedure to draw the fitted smooths on the original data, but predicting from each model at 200 equally spaced time points over the period of the data

[sourcecode language="r" gutter="false"]
plot(Annual ~ Year, data = gtemp, type = &quot;p&quot;, ylab = ylab)
pdat &lt;- with(gtemp,
             data.frame(Year = seq(min(Year), max(Year),
                        length = 200)))
p1 &lt;- predict(m1$gam, newdata = pdat)
p2 &lt;- predict(m2$gam, newdata = pdat)
lines(p1 ~ Year, data = pdat, col = &quot;red&quot;)
lines(p2 ~ Year, data = pdat, col = &quot;blue&quot;)
legend(&quot;topleft&quot;,
       legend = c(&quot;Uncorrelated Errors&quot;,&quot;AR(1) Errors&quot;),
       bty = &quot;n&quot;, col = c(&quot;red&quot;,&quot;blue&quot;), lty = 1)
[/sourcecode]

We can see that the AR(1) model is smoother than the original model. The AR(1) has absorbed some of the variation explained by the spline (trend) in the original and highlights an important point when fitting additive models to non-independent data; the fitted model may be overly complex and over fitted to the data if we do not account for the violation of independence in the residuals.

[caption id="attachment_152" align="aligncenter" width="600" caption="Comparison of the two fitted additive models"]<a href="http://ucfagls.files.wordpress.com/2011/06/comparison_of_gamm_models.png"><img src="http://ucfagls.files.wordpress.com/2011/06/comparison_of_gamm_models.png" alt="Comparison of the two fitted additive models" title="Comparison of fitted additive models" width="600" height="400" class="size-full wp-image-152" /></a>[/caption]

Having fitted a model, we can start to use it and interrogate it for a variety of purposes. One key question we might ask of the model is when were temperatures statistically significantly increasing (or decreasing for that matter)?

An approach answering this question is to compute the first derivatives of the fitted trend. We don't have an analytical form for the derivatives easily to hand, but we can use the method of finite differences to compute them. To produce derivatives via finite differences, we compute the values of the fitted trend at a grid of points over the entire data. We then shift the grid by a tiny amount and recompute the values of the trend at the new locations. The differences between the two sets of fitted values are the first differences of the trend and give a measure of the slope of the trend at any point in time.

The computations are not too involved and have been incorporated into a <code>Deriv()</code> function. We evaluate the trend at 200 equally spaced points. This function has a <code>plot()</code> method that draws a time series of first derivatives with a confidence interval. Periods where zero is not included in confidence interval can be coloured to show important periods of change (red for decreasing, and blue for increasing). The <code>sizer</code> argument turns on/off the colouring and <code>alpha</code> determines the coverage for the confidence interval.

[sourcecode language="r" gutter="false"]
m2.d &lt;- Deriv(m2, n = 200)
plot(m2.d, sizer = TRUE, alpha = 0.01)
[/sourcecode]

[caption id="attachment_154" align="aligncenter" width="600" caption="First derivatives of the additive model with AR(1) errors. A 99% point-wise confidence interval is shown. Periods where zero is not included in the confidence interval are periods of significant change are coloured red (decreasing) and blue (increasing)"]<a href="http://ucfagls.files.wordpress.com/2011/06/derivatives_of_fitted_model.png"><img src="http://ucfagls.files.wordpress.com/2011/06/derivatives_of_fitted_model.png" alt="First derivatives of the additive model with AR(1) errors. A 99% point-wise confidence interval is shown. Periods where zero is not included in the confidence interval are periods of significant change are coloured red (decreasing) and blue (increasing)" title="First derivatives of the additive model with AR(1) errors." width="600" height="400" class="size-full wp-image-154" /></a>[/caption]

We can manipulate the output from the <code>Deriv()</code> function to superimpose periods of significant change in temperature, as shown above on the first derivative plot, on the fitted trend:

[sourcecode language="r" gutter="false"]
plot(Annual ~ Year, data = gtemp, type = &quot;p&quot;, ylab = ylab)
lines(p2 ~ Year, data = pdat)
CI &lt;- confint(m2.d, alpha = 0.01)
S &lt;- signifD(p2, m2.d$Year$deriv, CI$Year$upper, CI$Year$lower,
             eval = 0)
lines(S$incr ~ Year, data = pdat, lwd = 3, col = &quot;blue&quot;)
lines(S$decr ~ Year, data = pdat, lwd = 3, col = &quot;red&quot;)
[/sourcecode]

The resulting figure is shown below:

[caption id="attachment_155" align="aligncenter" width="600" caption="Fitted additive model with AR(1) errors and superimposed periods of significant change in temperature"]<a href="http://ucfagls.files.wordpress.com/2011/06/fitted_additive_model_plus_derivatives.png"><img src="http://ucfagls.files.wordpress.com/2011/06/fitted_additive_model_plus_derivatives.png" alt="Fitted additive model with AR(1) errors and superimposed periods of significant change in temperature" title="Fitted additive model with AR(1) errors and superimposed periods of significant change in temperature" width="600" height="400" class="size-full wp-image-155" /></a>[/caption]

The derivatives suggest two periods of significant increase in temperature (at the 99% level); during the inter-war years and post ~1975. The second period of significant increase in global annual mean temperature appears to persist until ~2005. After that time, we have insufficient data to distinguish the fitted increasing trend from a zero-trend post 2005. It would be wrong to interpret the lack of significant change during periods where the fitted trend is either increasing or decreasing as gospel truth that the globe did or did not warm/cool. All we can say is that <em>given this sample of data</em>, we are unable to detect any further periods of significant change in temperature other than the two periods indicated in blue. This is because our estimate of the trend is subject to uncertainty.

Another observation worth making is that the fitted spline is based on the ML estimates of the coefficients that describe the spline. Each of these coefficients is subject to uncertainty, just as the regression coefficients in the <a href="http://ucfagls.wordpress.com/2011/06/11/global-warming-since-1995-now-significant/" title="Global warming since 1995 ‘now&nbsp;significant’">previous posting</a>. The set of coefficients and their standard errors form a multivariate normal distribution, from which we can sample new values of the coefficients that are <em>consistent</em> with the fitted model but will describe slightly different splines through the data and consequently, slightly different trends.

The <strong>MASS</strong> package contains function <code>mvrnorm()</code>, which allows us to draw samples from a multivariate normal distribution initialized using the model coefficients (<code>coef(m2$gam)</code>) and the variance-covariance matrix of the coefficients (<code>vcov(m2$gam)</code>). We set a seed for the random number generator to make the results reproducible, and take 1000 draws from this distribution

[sourcecode language="r" gutter="false"]
## simulate from posterior distribution of beta
Rbeta &lt;- mvrnorm(n = 1000, coef(m2$gam), vcov(m2$gam))
Xp &lt;- predict(m2$gam, newdata = pdat, type = &quot;lpmatrix&quot;)
sim1 &lt;- Xp %*% t(Rbeta)
[/sourcecode]

The $latex X_{p}$ matrix is a matrix such that when multiplied by the vector of model parameters it yields values of the linear predictor of the model. In other words, $latex X_{p}$ defines the parametrisation of the spline, which when multiplied by the model coefficients yields the fitted values of the model. <code>Rbeta</code> contains a matrix of coefficients that sample the uncertainty in the model. A matrix multiplication of the $latex X_{p}$ matrix with the coefficient matrix generates a matrix of fitted values of the trend, each column pertaining to a single version of the trend.

Next, I select, at random, 25 of these trends to illustrate the sorts of variation in the fitted trends

[sourcecode language="r" gutter="false"]
## plot the observation and 25 of the 1000 trends
set.seed(321)
want &lt;- sample(1000, 25)
ylim &lt;- range(sim1[,want], gtemp$Annual)
plot(Annual ~ Year, data = gtemp, ylim = ylim, ylab = ylab)
matlines(pdat$Year, sim1[,want], col = &quot;black&quot;, lty = 1, pch = NA)
[/sourcecode]

[caption id="attachment_157" align="aligncenter" width="600" caption="Examples of trends, each consistent with the fitted model, that illustrate the variation in the fitted trend due to uncertainty in the model parameter estimates. 25 such trends are shown"]<a href="http://ucfagls.files.wordpress.com/2011/06/25_simulated_trends.png"><img src="http://ucfagls.files.wordpress.com/2011/06/25_simulated_trends.png" alt="Examples of trends, each consistent with the fitted model, that illustrate the variation in the fitted trend due to uncertainty in the model parameter estimates" title="Examples of trends, each consistent with the fitted model, that illustrate the variation in the fitted trend due to uncertainty in the model parameter estimates" width="600" height="400" class="size-full wp-image-157" /></a>[/caption]

What do simulated trends suggest for the most recent period that has been the interest of many? The following code focusses on the post 1990 data and shows 50 of the simulated trends

[sourcecode language="r" gutter="false"]
set.seed(321)
want &lt;- sample(1000, 50)
rwant &lt;- with(pdat, which(Year &gt;= 2000))
twant &lt;- with(gtemp, which(Year &gt;= 2000))
ylim &lt;- range(sim1[rwant,want], gtemp$Annual[twant])
plot(Annual ~ Year, data = gtemp, ylim = ylim,
     xlim = c(1990, 2009), type = &quot;n&quot;, ylab = ylab)
matlines(pdat$Year, sim1[,want], col = &quot;black&quot;, lty = 1, pch = NA)
points(Annual ~ Year, data = gtemp, col = &quot;red&quot;, bg = &quot;yellow&quot;,
       pch = 21, cex = 1.5)
[/sourcecode]

which produces the following figure

[caption id="attachment_162" align="aligncenter" width="600" caption="50 simulated trends from the fitted additive model for the period 1990&ndash;2010. The yellow and red points are the observed mean annual temperatures."]<a href="http://ucfagls.files.wordpress.com/2011/06/simulated_trends_recent_period.png"><img src="http://ucfagls.files.wordpress.com/2011/06/simulated_trends_recent_period.png" alt="50 simulated trends from the fitted additive model for the period 1990&ndash;2010" title="50 simulated trends from the fitted additive model for the period 1990&ndash;2010" width="600" height="400" class="size-full wp-image-162" /></a>[/caption]

A couple of the simulated trends are suggestive of a decreasing decreasing trend over the period, whilst a number suggest that the temperature increase has stalled. However, the majority of the simulated trends suggest that the temperature increase continues throughout the recent period though perhaps with reduced slope, and this is consistent with the fitted trend which also increases throughout this period. The range of trends, particularly at the very end of the observation period reflects the large degree of uncertainty in the trend at the edge of the data; we simply do not have the data available to constrain our estimates of the trend at the end of the observation period.

In summary, by using a model that is fitted to the entire period of data but which can adapt to local features of the time series provides a powerful means of estimating trends in temperature data. The thin-plate spline that describes the fitted trend is defined by a set of coefficients that we can use to explore the uncertainty in the model via simulation. Because the model can be expressed as a linear mixed model we can exploit the <code>lme()</code> function to fit correlation structures in the model residuals to account for the autocorrelation in the data.
