--- 
title: Smoothing temporally correlated data
status: publish
layout: post
published: true
meta: 
  _edit_last: "15232487"
  jabber_published: "1311266113"
type: post
tags: 
- R
- Time series
---
Something I have been doing a lot of work with recently are time series data, to which I have been fitting additive models to describe trends and other features of the data. When modelling temporally dependent data, we often need to adjust our fitted models to account for the lack of independence in the model residuals. When smoothing such data, however, there is an additional problem that needs to be addressed when we are determining the complexity of the fitted smooths as part of the model fit. Unless we specifically tell the software that the data aren't independent it will perform smoothness selection assuming that we have $latex n$ independent observations. The risk then is that too-complex a smooth term is fitted to the data &mdash; it is no-longer a case of updating the fitted model, the model itself will be over-fitted. In this post I want to illustrate the problem of smoothing correlated data with an example from a chapter in a text book that a reviewer alerted to me to some time back
<!--more-->
The example comes from Kohn, Schimek and Smith (2000) that I have cooked up using R. Kohn et al consider the model $latex f(x_{t}) = 1280 x_{t}^4 (1 - x_{t})^4$, where $latex t = 1, 2, \ldots, 100$, and $latex x_{t} = t/100$. To this, errors $latex e_{t}$ are generated from a first-order auto-regressive (AR(1)) process with $latex \phi_{1} = 0.3713$ to produce a random sample from the model such that $latex y_{t} = f(x_{t}) + e_{t}$. We can generate a sample of data from this model with the following R code

[sourcecode language="r" gutter="false"]
set.seed(321)
n &lt;- 100
time &lt;- 1:n
xt &lt;- time/n
Y &lt;- (1280 * xt^4) * (1- xt)^4
y &lt;- as.numeric(Y + arima.sim(list(ar = 0.3713), n = n))
[/sourcecode]

The <code>arima.sim()</code> function is used to generate the appropriate AR(1) errors. A plot of this sample of data and the true function are shown below
[caption id="attachment_187" align="aligncenter" width="600" caption="True function and a random sample of 100 observations from this function with AR(1) errors"]<a href="http://ucfagls.files.wordpress.com/2011/07/schimek_example_1.png"><img src="http://ucfagls.files.wordpress.com/2011/07/schimek_example_1.png" alt="Random sample and true function as used by Kohn et al" title="smoothing_dependent_data_example_1" width="600" height="600" class="size-full wp-image-187" /></a>[/caption]

To these data, I will fit a cubic smoothing spline via <code>smooth.spline()</code> and an additive model via <code>gam()</code> in package <strong>mgcv</strong>. In addition, let us assume that we don't know the exact nature of the dependence in the data but we know that they are temporally correlated so that we can fit a model that includes a plausible correlation structure. For that, I will use an additive model with an AR(1) correlation structure, fitted using a linear mixed effects representation of the additive model via the <code>gamm()</code> function, also from the <strong>mgcv</strong> package. <code>gamm()</code> uses the <code>lme()</code> function from the <strong>nlme</strong> package. I will arrange for the value of $latex \phi_{1}$ be estimated as one of the model parameters, whilst the degree of smoothness is being estimated during fitting.

The three models are fitted with the following three lines of R code:

[sourcecode language="r" gutter="false"]
m1 &lt;- smooth.spline(xt, y)
m2 &lt;- gam(y ~ s(xt, k = 20))
m3 &lt;- gamm(y ~ s(xt, k = 20), correlation = corAR1(form = ~ time))
[/sourcecode]

The three model fits are shown in the figure below

[caption id="attachment_189" align="aligncenter" width="600" caption="The three resulting model fits to the Kohn et al example data set. Both the cubic smoothing spline and the standard additive model over-fit the data resulting in very complex fits using a large number of degrees of freedom. The AM with AR(1) errors accurately fits the underlying true function"]<a href="http://ucfagls.files.wordpress.com/2011/07/schimek_example_2.png"><img src="http://ucfagls.files.wordpress.com/2011/07/schimek_example_2.png" alt="The three resulting model fits to the Kohn et al example data set. Both the cubic smoothing spline and the standard additive model over fit the data resulting in very complex fits using a large number of degrees of freedom. The AM with AR(1) errors accurately fits the underlying true function" title="three_model_fits_to the kohn_et_al_example_data" width="600" height="600" class="size-full wp-image-189" /></a>[/caption]

Both the cubic smoothing spline and the additive model over fit the data, resulting in very complex smooth functions using 34.25 and 16.82 degrees of freedom respectively. The additive model with AR(1) errors does a very good job of retrieving the true function from which the data were generated, only really deviating from this function at low values of $latex x_{t}$ where there are few data to constrain the fit. The code used to produce the figure is shown below

[sourcecode language="r" gutter="false"]
edf2 &lt;- summary(m2)$edf
edf3 &lt;- summary(m3$gam)$edf

plot(y ~ xt, xlab = expression(x[t]), ylab = expression(y[t]))
lines(Y ~ xt, lty = &quot;dashed&quot;, lwd = 1)
lines(fitted(m1) ~ xt, lty = &quot;solid&quot;, col = &quot;darkolivegreen&quot;, lwd = 2)
lines(fitted(m2) ~ xt, lty = &quot;solid&quot;, col = &quot;red&quot;, lwd = 2)
lines(fitted(m3$lme) ~ xt, lty = &quot;solid&quot;, col = &quot;midnightblue&quot;, lwd = 2)
legend(&quot;topleft&quot;,
       legend = c(&quot;Truth&quot;,
       paste(&quot;Cubic spline (edf = &quot;, round(m1$df, 2), &quot;)&quot;, sep = &quot;&quot;),
       paste(&quot;AM (edf = &quot;, round(edf2, 2), &quot;)&quot;, sep = &quot;&quot;),
       paste(&quot;AM + AR(1) (edf = &quot;, round(edf3, 2), &quot;)&quot;, sep = &quot;&quot;)),
       col = c(&quot;black&quot;, &quot;darkgreen&quot;, &quot;red&quot;, &quot;midnightblue&quot;),
       lty = c(&quot;dashed&quot;, rep(&quot;solid&quot;, 3)),
       lwd = c(1, rep(2, 3)),
       bty = &quot;n&quot;, cex = 0.8)
[/sourcecode]

The <code>intervals()</code> function can be used to extract the estimate for $latex \phi_{1}$ and a 95% confidence interval on the estimate:

[sourcecode language="r" gutter="false"]
&gt; intervals(m3$lme, which = &quot;var-cov&quot;) ## edited for brevity
....
 Correlation structure:
        lower      est.     upper
Phi 0.1705591 0.4032966 0.5934125
attr(,&quot;label&quot;)
[1] &quot;Correlation structure:&quot;
....
[/sourcecode]

Despite being somewhat imprecise, the estimate, $latex \hat{\phi}_{1} = 0.4033$, is very close to the known values used to generate the sample of data.

Whilst being a little contrived (I purposely increased the basis dimension on the basic additive model to <code>k = 20</code> [otherwise the fit with the default <code>k</code> is close the model with AR(1) errors!], and use GCV smoothness selection rather than the better performing ML or REML methods available in <code>gam()</code>), the example shows quite nicely the problems associated with smoothness selection when fitting additive model to dependent data. If you know something about the system under study and the sort of variation in the data one might expect to observe, an alternative approach to fitting an additive model to dependent data would be to fix the smoothness at an appropriate, low value. To perform any subsequent inference on the fitted model, we would have to estimate a correlation matrix from the residuals of that model using a time series model and use that to update the covariance matrix of the fitted additive model. I'm still working on how to do that last bit with <code>gam()</code> and <strong>mgcv</strong>.

<h2>References</h2>
Kohn R., Schimek M.G., Smith M. (2000) Spline and kernel regression for dependent data. In Schimekk M.G. (Ed) (2000) <em>Smoothing and Regression: approaches, computation and application</em>. John Wiley &amp; Sons, Inc.
